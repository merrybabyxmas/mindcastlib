# crawling_utils.py
from __future__ import annotations

# =========================
# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
# =========================
import os
import json
import time
import re
import datetime as dt
from pathlib import Path
from urllib.parse import urlparse, parse_qs, urlunparse, urlencode

# =========================
# ì„œë“œíŒŒí‹°
# =========================
import requests
import numpy as np
import pandas as pd

# =========================
# ì „ì—­ ì„¤ì •
# =========================
PROJECT_ROOT = Path(__file__).resolve().parents[1] # for ì ˆëŒ€ê²½ë¡œ  
METADATA_PATH = PROJECT_ROOT / "data_suicide_crawling" / "metadata.json"
# ============================================================
# ğŸ“¦ file_utils
# ============================================================
def ensure_parent_dir(path: str):
    Path(path).parent.mkdir(parents=True, exist_ok=True)

# ============================================================
# ğŸ“¦ metadata ê´€ë¦¬
# ============================================================
def update_meta(meta_path: str, key: str, record: dict):
    p = Path(meta_path)
    p.parent.mkdir(parents=True, exist_ok=True)

    if p.exists():
        meta = json.loads(p.read_text(encoding="utf-8"))
    else:
        meta = {}

    rec = dict(record)
    rec["collected_at"] = dt.datetime.now().isoformat(timespec="seconds")
    meta[key] = rec

    p.write_text(
        json.dumps(meta, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )

# ============================================================
# ğŸ“¦ collectors.common
# ============================================================
def _set_query(url: str, **kwargs) -> str: # urlì˜ ì¿¼ë¦¬ ë¶€ë¶„ì„ ê°€ì ¸ì™€ì„œ ì¿¼ë¦¬ë¥¼ ë°”ê¾¼ urlì„ return í•©ë‹ˆë‹¤
    u = urlparse(url)
    q = parse_qs(u.query)
    for k, v in kwargs.items():
        q[k] = [str(v)]
    new_q = urlencode(q, doseq=True)
    return urlunparse((u.scheme, u.netloc, u.path, u.params, new_q, u.fragment))

# 
def _add_months(d: dt.date, months: int) -> dt.date:
    y = d.year + (d.month - 1 + months) // 12
    m = (d.month - 1 + months) % 12 + 1
    return dt.date(y, m, 1)

def iter_ym_chunks_6m(start_ym: str, end_ym: str):
    """
    start_ym/end_ym: 'YYYYMM'
    ë°ì´í„°ë¥¼ 6ê°œì›” ë‹¨ìœ„ë¡œ ëŠì–´ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    s = dt.date(int(start_ym[:4]), int(start_ym[4:]), 1)
    e = dt.date(int(end_ym[:4]), int(end_ym[4:]), 1)

    cur = s
    while cur <= e:
        nxt = _add_months(cur, 6)         # ë‹¤ìŒ ì²­í¬ ì‹œì‘(6ê°œì›” ë’¤)
        chunk_end = _add_months(nxt, -1)  # ì²­í¬ ë = ë‹¤ìŒ ì‹œì‘ì˜ ì „ì›”
        if chunk_end > e:
            chunk_end = e

        yield f"{cur.year:04d}{cur.month:02d}", f"{chunk_end.year:04d}{chunk_end.month:02d}"
        cur = nxt

def make_latest_dated_path(base_path: str, date: str | None = None) -> str:

    if date is None:
        date = dt.date.today().strftime("%Y%m%d")

    p = Path(base_path)
    return str(p.with_name(f"{p.stem}_{date}{p.suffix}"))

def fetch_kosis_by_6m(openapi_url: str, start_ym: str, end_ym: str, fetch_to_df, sleep_s: float = 0.3) -> pd.DataFrame:
    frames = []
    for s, e in iter_ym_chunks_6m(start_ym, end_ym):
        url = _set_query(openapi_url, startPrdDe=s, endPrdDe=e)
        df = fetch_to_df(url)
        if df is None or df.empty:
            continue
        frames.append(df)
        time.sleep(sleep_s)  # ì„œë²„ ê³¼ë¶€í•˜/íƒ€ì„ì•„ì›ƒ ì™„í™”

    if not frames:
        return pd.DataFrame()

    out = pd.concat(frames, ignore_index=True)

    # ì¤‘ë³µ ì œê±°(ê¸°ê°„ ê²¹ì¹¨/ì„œë²„ ì¤‘ë³µ ì‘ë‹µ ëŒ€ë¹„)
    out = out.drop_duplicates()
    return out

def replace_latest_dated_file(base_path: str) -> str: # ê°±ì‹  ë‹¹ì¼ì˜ ë‚ ì§œë¥¼ ê¸°ì¡´ íŒŒì¼ëª… ë’¤ì— ë¶™ì…ë‹ˆë‹¤
    """
    ê°™ì€ ë””ë ‰í„°ë¦¬ ë‚´ì˜ *_latest_*.csv ë¥¼ ëª¨ë‘ ì‚­ì œí•˜ê³ 
    ì˜¤ëŠ˜ ë‚ ì§œì˜ *_latest_YYYYMMDD.csv ê²½ë¡œë¥¼ ë°˜í™˜
    """
    p = Path(base_path)
    pattern = f"{p.stem}_*{p.suffix}"   

    # ê¸°ì¡´ latest_ë‚ ì§œ íŒŒì¼ ì „ë¶€ ì‚­ì œ
    for f in p.parent.glob(pattern):
        f.unlink(missing_ok=True)

    # ìƒˆ íŒŒì¼ ê²½ë¡œ ìƒì„±
    return make_latest_dated_path(base_path)

def build_url_with_dynamic_period(issued_url: str, start_ym: str) -> str:
    """ë°œê¸‰ URLì„ í…œí”Œë¦¿ìœ¼ë¡œ ì‚¬ìš©í•˜ë˜ start/endë§Œ ë™ì ìœ¼ë¡œ"""
    u = urlparse(issued_url)
    q = parse_qs(u.query)

    today = dt.date.today()
    end_ym = f"{today.year}{today.month:02d}"

    q["startPrdDe"] = [start_ym]
    q["endPrdDe"] = [end_ym]

    new_query = urlencode(q, doseq=True)
    return urlunparse((u.scheme, u.netloc, u.path, u.params, new_query, u.fragment))

def fetch_to_df(url: str) -> pd.DataFrame:
    r = requests.get(url, timeout=30)
    r.raise_for_status()

    data = r.json()

    # âœ…  ì •ìƒì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ì„ ê²½ìš° list[dict]
    if isinstance(data, list):
        return pd.DataFrame(data)

    # âœ… 2) dict ì¼€ì´ìŠ¤: ì—ëŸ¬ or ë˜í•‘ëœ ê²°ê³¼
    if isinstance(data, dict):
        # (A) KOSIS "ë°ì´í„° ì—†ìŒ"ì€ err=30 â†’ ì˜ˆì™¸ê°€ ì•„ë‹ˆë¼ ë¹ˆ DFë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ -> fetch_6mì„ ìœ„í•´ì„œ
        if str(data.get("err", "")).strip() == "30":
            return pd.DataFrame()

        # (B) ê·¸ ì™¸ KOSIS ì—ëŸ¬ëŠ” raise
        # - err / error / message ë“±ì´ ìˆìœ¼ë©´ ì—ëŸ¬ë¡œ ê°„ì£¼
        err_keys = ["err", "errMsg", "error", "message"]
        if any(k in data for k in err_keys):
            raise ValueError(f"KOSIS API returned error dict: {data}")

        # (C) dict ì•ˆì— listê°€ ë“¤ì–´ìˆëŠ” í˜•íƒœ
        for key in ["data", "items", "rows", "list", "result", "RESULT"]:
            if key in data and isinstance(data[key], list):
                return pd.DataFrame(data[key])

        # (D) (ë””ë²„ê¹…ìš©)
        return pd.DataFrame([data])

    raise ValueError(f"Unexpected JSON response type: {type(data)}")
# ============================================================
# ğŸ“¦ parser.year_to_month
# ============================================================
def expand_year_to_months(df_year, year_col="date", value_cols=None):
    """
    ì—°ë„ ë°ì´í„° -> ì›”ë³„ ë°ì´í„° (ê°’ ë‹¨ìˆœ ë³µì œ)
    """
    if value_cols is None:
        value_cols = [c for c in df_year.columns if c != year_col]


    rows = []
    for _, row in df_year.iterrows():
        y = row[year_col]

        # year ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        if isinstance(y, pd.Timestamp):
            year = y.year
        else:
            year = int(str(y)[:4])  # "2020", "2020-01" ê°™ì€ ê²ƒë„ ì²˜ë¦¬

        for m in range(1, 13):
            new_row = {"date": f"{year}-{m:02d}"}
            for col in value_cols:
                new_row[col] = row[col]
            rows.append(new_row)

    df_month = pd.DataFrame(rows)
    return df_month

# ============================================================
# ğŸ“¦ parser.apply_denton
# ============================================================
def build_A(T, m=3):
    """
    ì œì•½í–‰ë ¬ A: ë¶„ê¸° -> ì›” í•©ê³„ ë³´ì¡´
    """
    A = np.zeros((T, T * m))
    for i in range(T):
        A[i, i*m:(i+1)*m] = 1
    return A


def build_D(n):
    """
    1ì°¨ ì°¨ë¶„ í–‰ë ¬ D
    """
    D = np.zeros((n - 1, n))
    for i in range(n - 1):
        D[i, i] = -1
        D[i, i + 1] = 1
    return D

def quarter_label_to_months(q_label: str):
    # ë¶„ê¸° ë¼ë²¨ -> ì›” ë¼ë²¨ 
    year, q = q_label.split("-")
    q = int(q)

    if q not in [1, 2, 3, 4]:
        raise ValueError(f"Invalid quarter label: {q_label}")

    start_month = (q - 1) * 3 + 1  # 1,4,7,10

    months = []
    for m in range(start_month, start_month + 3):
        months.append(f"{year}-{m:02d}")

    return months


def apply_denton(y_quarterly, m=3):
    y = np.asarray(y_quarterly).reshape(-1) # ì›ë˜ ì¿¼í„° ë°ì´í„°
    T = len(y)
    n = T * m # ìƒì„±í•´ì•¼í•  ì›” ê°œìˆ˜ 

    A = build_A(T, m)
    D = build_D(n)
    DTD = D.T @ D

    zero = np.zeros((A.shape[0], A.shape[0]))
    # ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²• ì ìš© í›„ ì—°ë¦½ë°©ì •ì‹ ì‹ ë„ì¶œ 
    KKT = np.block([
        [DTD, A.T],
        [A, zero]
    ]) # ì¢Œë³€ ì •ì˜ 

    rhs = np.concatenate([np.zeros(n), y]) #ìš°ë³€ ì •ì˜

    sol = np.linalg.solve(KKT, rhs) # ì—°ë¦½ë°©ì •ì‹ í’€ì´ 
    x_monthly = sol[:n]

    return x_monthly

def denton_with_dates(df_quarter, date_col="date", value_cols="value" ): 
    """
    df_quarter: ë¶„ê¸° ë°ì´í„° (ì—¬ëŸ¬ ì§€í‘œ ì»¬ëŸ¼)
    date_col: ë¶„ê¸° ë¼ë²¨ ì»¬ëŸ¼
    value_cols: Denton ì ìš©í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ date_col ì œì™¸ ì „ë¶€)
    """
    df_quarter = df_quarter.sort_values(date_col).reset_index(drop=True)

    if value_cols is None:
        value_cols = [c for c in df_quarter.columns if c != date_col]

    # ë¶„ê¸° -> ì›” ì¸ë±ìŠ¤ ìƒì„±
    month_dates = []
    for q_label in df_quarter[date_col]:
        month_dates.extend(quarter_label_to_months(q_label))

    df_month = pd.DataFrame({"date": month_dates})

    # ì»¬ëŸ¼ë³„ë¡œ Denton ì ìš©
    for col in value_cols:
        y = pd.to_numeric(df_quarter[col], errors="raise").values
        x_month = apply_denton(y)
        df_month[col] = x_month
    return df_month
    
# ============================================================
# ğŸ“¦ collectors êµ¬í˜„ë¶€
# ============================================================

def cpi_run(cfg: dict):
     # 1) ìˆ˜ì§‘
    url = build_url_with_dynamic_period(cfg["openapi_url"], cfg.get("start_ym", "196501"))
    raw = fetch_to_df(url)

    # 2) ì „ì²˜ë¦¬(collectorì— í¬í•¨)
    df = raw[["PRD_DE", "DT"]].copy()
    df.columns = df.columns.astype(str).str.strip()
    df = df.rename(columns={"PRD_DE": "date", "DT": "cpi"})

    df["cpi"] = pd.to_numeric(df["cpi"], errors="coerce")
    df["date"] = pd.to_datetime(df["date"], format="%Y%m").dt.strftime("%Y-%m")
    df = df.sort_values("date").reset_index(drop=True)
    
    # 3) ì €ì¥
    out_csv = PROJECT_ROOT / cfg["output_csv"]
    ensure_parent_dir(out_csv)
    # 1) ì´ì „ latest_ë‚ ì§œ íŒŒì¼ ì œê±° + ì˜¤ëŠ˜ íŒŒì¼ ê²½ë¡œ ìƒì„±
    out_csv = replace_latest_dated_file(out_csv)
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # 4) metadata ê¸°ë¡
    key = cfg.get("metadata_key", "cpi")
    update_meta(METADATA_PATH, key, {
        "saved_file": out_csv,
        "source_url": url,
        "rows": int(df.shape[0]),
        "max_date": df["date"].max(),
    })

    print("âœ… CPI ì €ì¥:", out_csv, "rows:", len(df), "max_date:", df["date"].max())


def consumer_price_change_index_run(cfg: dict):
    # 1) ìˆ˜ì§‘
    url = build_url_with_dynamic_period(cfg["openapi_url"], cfg.get("start_ym",))
    raw = fetch_to_df(url)
    
    # 2) ì „ì²˜ë¦¬(collectorì— í¬í•¨)
    raw["C1_NM"] = raw["C1_NM"].astype(str).str.strip()
    raw = raw[raw["C1_NM"].eq("ì´ì§€ìˆ˜")].copy()

    df = raw[["PRD_DE", "DT"]].copy()
    df.columns = df.columns.astype(str).str.strip()
    df = df.rename(columns={
    "PRD_DE": "date",
    "DT": "ì†Œë¹„ìë¬¼ê°€ë“±ë½ë¥ "
    })

    if df["date"].str.len().iloc[0] == 6:
        df["date"] = pd.to_datetime(df["date"], format="%Y%m").dt.strftime("%Y-%m")
    else:
        df["date"] = pd.to_datetime(df["date"], format="%Y").dt.strftime("%Y")

    df  = expand_year_to_months(df, year_col="date", value_cols = None) # ì—°ê°„ë°ì´í„°ë¥¼ ë³µì‚¬í•´ì„œ ì›”ë³„ë°ì´í„°ë¡œ ë³€í™˜ 
    # 3) ì €ì¥
    out_csv = PROJECT_ROOT / cfg["output_csv"]
    ensure_parent_dir(out_csv)
    # 1) ì´ì „ latest_ë‚ ì§œ íŒŒì¼ ì œê±° + ì˜¤ëŠ˜ íŒŒì¼ ê²½ë¡œ ìƒì„±
    out_csv = replace_latest_dated_file(out_csv)
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # 4) metadata ê¸°ë¡
    key = cfg.get("metadata_key", "consumer_price_change_index")
    update_meta(METADATA_PATH, key, {
        "saved_file": out_csv,
        "source_url": url,
        "rows": int(df.shape[0]),
        "max_date": df["date"].max(),
    })

    print("âœ… Consumer_Price_Change_Index ì €ì¥:", out_csv, "rows:", len(df), "max_date:", df["date"].max())


def average_working_day_run(cfg: dict):
   
    url =cfg["openapi_url"] #ì§€í‘œëˆ„ë¦¬ apiëŠ” í•­ìƒ ìµœì‹  ê°’ì„ ë°˜í™˜ 
    df = fetch_to_df(url)
    df.columns = df.columns.astype(str).str.strip()
    
    date_col =  "ì‹œì "
    value_col =  "ê°’"
    item_col = "í•­ëª©ì´ë¦„"
    df = df[df[date_col].str.len() == 6].copy() # monthly ì§€í‘œë§Œ ë°›ì•„ì˜¤ê²Œ ë©ë‹ˆë‹¤

    df = df[[date_col,item_col,value_col]].copy()
    df = df[df[item_col].isin(["ê·¼ë¡œì¼ìˆ˜"])].copy()

    df[value_col] = pd.to_numeric(df[value_col],errors= "coerce")
    df[item_col] = df[item_col].astype(str).str.strip()
    df[date_col] = df[date_col].astype(str).str.strip()


    wide = df.pivot_table(
        index="ì‹œì ",
        columns=item_col,
        values=value_col,
        aggfunc="first"
    ).sort_index()
    wide = wide.reset_index()
  
    wide = wide.rename(columns = {"ì‹œì ":"date"})
    if wide["date"].str.len().iloc[0] == 6:
        wide["date"] = pd.to_datetime(wide["date"], format="%Y%m").dt.strftime("%Y-%m")
    else:
        wide["date"] = pd.to_datetime(wide["date"], format="%Y").dt.strftime("%Y")
        
    out_csv = PROJECT_ROOT / cfg["output_csv"]
    ensure_parent_dir(out_csv)
    # 1) ì´ì „ latest_ë‚ ì§œ íŒŒì¼ ì œê±° + ì˜¤ëŠ˜ íŒŒì¼ ê²½ë¡œ ìƒì„±
    out_csv = replace_latest_dated_file(out_csv)
    wide.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # 4) metadata ê¸°ë¡
    key = cfg.get("metadata_key", "average_working_day")
    update_meta(METADATA_PATH, key, {
        "saved_file": out_csv,
        "source_url": url,
        "rows": int(wide.shape[0]),
        "max_date": wide["date"].max(),
    })

    print("âœ… Average_Working_Day ì €ì¥:", out_csv, "rows:", len(wide), "max_date:", wide["date"].max())
   

def aver_mid_age_run(cfg: dict):
    url = build_url_with_dynamic_period(cfg["openapi_url"], cfg.get("start_ym",))
    raw = fetch_to_df(url)
    

    # ì „ì²˜ë¦¬
    raw.columns = raw.columns.astype(str).str.strip()
    raw = raw[raw["C2_NM"] == "ì „êµ­"].copy()

    date_col = "PRD_DE"
    item_col =  "ITM_NM"
    value_col = "DT"

    raw= raw[[date_col,item_col,value_col]].copy()
    
    raw = raw[raw[item_col].isin(["ì¤‘ìœ„ì—°ë ¹", "í‰ê· ì—°ë ¹"])].copy()
    
    
    raw[value_col] = pd.to_numeric(raw[value_col],errors= "coerce")
    raw[item_col] = raw[item_col].astype(str).str.strip()
    raw[date_col] = raw[date_col].astype(str).str.strip()
    
    wide = raw.pivot_table(
        index=date_col,
        columns=item_col,
        values=value_col,
        aggfunc="first"
    ).sort_index()
    wide = wide.reset_index()
    wide = wide.rename(columns = {date_col : "date"})

    
    if wide["date"].str.len().iloc[0] == 6:
        wide["date"] = pd.to_datetime(wide["date"], format="%Y%m").dt.strftime("%Y-%m")
    else:
        wide["date"] = pd.to_datetime(wide["date"], format="%Y").dt.strftime("%Y")

    wide = expand_year_to_months(wide ,year_col="date",value_cols=None) # ì—° ë°ì´í„° ë³µì‚¬í•´ì„œ ì›”ë³„ë¡œ 
    # 3) ì €ì¥
    out_csv = PROJECT_ROOT / cfg["output_csv"]
    ensure_parent_dir(out_csv)
    # 3-1) ì´ì „ latest_ë‚ ì§œ íŒŒì¼ ì œê±° + ì˜¤ëŠ˜ íŒŒì¼ ê²½ë¡œ ìƒì„±
    out_csv = replace_latest_dated_file(out_csv)
    wide.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # 3-2) metadata ê¸°ë¡
    key = cfg.get("metadata_key", "aver_mid_age")
    update_meta(METADATA_PATH, key, {
        "saved_file": out_csv,
        "source_url": url,
        "rows": int(wide.shape[0]),
        "max_date": wide["date"].max(),
    })

    print("âœ… Aver_Mid_Age ì €ì¥:", out_csv, "rows:", len(wide), "max_date:", wide["date"].max())


def loan_run(cfg: dict):
    start_ym = cfg.get("start_ym", "200204")
    url = build_url_with_dynamic_period(cfg["openapi_url"], start_ym)
    df = fetch_to_df(url)
    df.columns = df.columns.astype(str).str.strip()

    date_col = "PRD_DE"
    value_col = "DT"
    item_col = "C1_NM"

    # 2) ì „ì²˜ë¦¬
    df = df[[date_col, item_col, value_col]].copy()

    wanted = ["ê°€ê³„ì‹ ìš©", "ê°€ê³„ëŒ€ì¶œ", "íŒë§¤ì‹ ìš©"]
    df[item_col] = df[item_col].astype(str).str.strip()
    df = df[df[item_col].isin(wanted)].copy()

    df[value_col] = pd.to_numeric(df[value_col], errors="coerce")
    df[date_col] = df[date_col].astype(str).str.strip()
    

    wide = (
        df.pivot_table(index=date_col, columns=item_col, values=value_col, aggfunc="first")
          .sort_index()
    )
    wide = wide[[c for c in wanted if c in wide.columns]].reset_index()
    wide = wide.rename(columns={date_col: "date"})
    wide["date"] = pd.to_datetime(wide["date"], format="%Y%m").dt.strftime("%Y-%m")
    wide = denton_with_dates(wide ,date_col="date",value_cols=None) ## denton ì¶”ê°€ 
    # 3) ì €ì¥
    out_csv = PROJECT_ROOT / cfg["output_csv"]
    ensure_parent_dir(out_csv)
    # 1) ì´ì „ latest_ë‚ ì§œ íŒŒì¼ ì œê±° + ì˜¤ëŠ˜ íŒŒì¼ ê²½ë¡œ ìƒì„±
    out_csv = replace_latest_dated_file(out_csv)
    wide.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # 4) metadata ê¸°ë¡
    key = cfg.get("metadata_key", "loan")
    update_meta(METADATA_PATH, key, {
        "saved_file": out_csv,
        "source_url": url,
        "rows": int(wide.shape[0]),
        "max_date": wide["date"].max() if not wide.empty else None,
    })

    print("âœ… Loan ì €ì¥:", out_csv, "rows:", len(wide), "max_date:", wide["date"].max() if not wide.empty else None)
    


def gdp_gni_run(cfg: dict):
    
    url = build_url_with_dynamic_period(cfg["openapi_url"], cfg.get("start_ym",))
    raw = fetch_to_df(url)
    
    
    # ì „ì²˜ë¦¬
    raw.columns = raw.columns.astype(str).str.strip()
   
    date_col = "PRD_DE"
    item_col =  "C1_NM"
    value_col = "DT"

    raw= raw[[date_col,item_col,value_col]].copy()
    raw = raw[raw[item_col].isin(["êµ­ë‚´ì´ìƒì‚°(ì‹œì¥ê°€ê²© GDP)", "êµ­ë¯¼ì´ì†Œë“(GNI)"])].copy()
    
    raw[value_col] = pd.to_numeric(raw[value_col],errors= "coerce")
    raw[item_col] = raw[item_col].astype(str).str.strip()
    raw[date_col] = raw[date_col].astype(str).str.strip()


    wide = raw.pivot_table(
        index=date_col,
        columns=item_col,
        values=value_col,
        aggfunc="first"
    ).sort_index()
    wide = wide.reset_index()
    wide = wide.rename(columns = {date_col : "date"})

    
    if wide["date"].str.len().iloc[0] == 6:
        wide["date"] = pd.to_datetime(wide["date"], format="%Y%m").dt.strftime("%Y-%m")
    else:
        wide["date"] = pd.to_datetime(wide["date"], format="%Y").dt.strftime("%Y")
    
    wide = denton_with_dates(wide ,date_col="date",value_cols=None) ## denton ì¶”ê°€ 
    # 3) ì €ì¥
    out_csv = PROJECT_ROOT / cfg["output_csv"]
    ensure_parent_dir(out_csv)
    # 3-1) ì´ì „ latest_ë‚ ì§œ íŒŒì¼ ì œê±° + ì˜¤ëŠ˜ íŒŒì¼ ê²½ë¡œ ìƒì„±
    out_csv = replace_latest_dated_file(out_csv)
    wide.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # 3-2) metadata ê¸°ë¡
    key = cfg.get("metadata_key", "gdp_gni")
    update_meta(METADATA_PATH, key, {
        "saved_file": out_csv,
        "source_url": url,
        "rows": int(wide.shape[0]),
        "max_date": wide["date"].max(),
    })

    print("âœ… GDP_GNI ì €ì¥:", out_csv, "rows:", len(wide), "max_date:", wide["date"].max())


def normalize_item(name: str) -> str:
    s = str(name).strip()
    if "ì‹¤ì—…ë¥ " in s: return "ì‹¤ì—…ë¥ "
    if "ê³ ìš©ë¥ " in s: return "ê³ ìš©ë¥ "
    if "ê²½ì œí™œë™ì°¸ê°€ìœ¨" in s: return "ê²½ì œí™œë™ì°¸ê°€ìœ¨"
    return s

def labor_force_run(cfg: dict):
     # 1) ìˆ˜ì§‘
    start_ym = cfg.get("start_ym",)
    url = build_url_with_dynamic_period(cfg["openapi_url"], start_ym)
    df = fetch_to_df(url)
    df.columns = df.columns.astype(str).str.strip()

    date_col = "PRD_DE"
    value_col = "DT"
    item_col = "ITM_NM"
    group_col = "C1_NM"
    if item_col not in df.columns:
        raise KeyError(f"ITM_NM ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. columns={list(df.columns)}")
    if group_col not in df.columns:
        raise KeyError("â€˜ê³„â€™ë¥¼ ë‹´ëŠ” ë¶„ë¥˜ ì»¬ëŸ¼(C1_NM ë“±)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    # 3) ê³„ë§Œ í•„í„°
    df2 = df[df[group_col].astype(str).str.strip().eq("ê³„")].copy()

    # 4) í•„ìš”í•œ í•­ëª©ë§Œ í•„í„° 
    df2[item_col] = df2[item_col].astype(str).str.strip()
    pat = "ê²½ì œí™œë™ì¸êµ¬|ë¹„ê²½ì œí™œë™ì¸êµ¬|ì·¨ì—…ì|ì‹¤ì—…ì|ê²½ì œí™œë™ì°¸ê°€ìœ¨|ì‹¤ì—…ë¥ |ê³ ìš©ë¥ "
    df2 = df2[df2[item_col].str.contains(pat, na=False)].copy()

    # 5) íƒ€ì… ì •ë¦¬
    df2[value_col] = pd.to_numeric(df2[value_col], errors="coerce")
    df2[date_col] = df2[date_col].astype(str).str.strip()

    # 6) ë‚ ì§œ ì •ë¦¬
    if df2[date_col].str.len().iloc[0] == 6:
        df2["date"] = pd.to_datetime(df2[date_col], format="%Y%m", errors="coerce").dt.strftime("%Y-%m")
    else:
        df2["date"] = pd.to_datetime(df2[date_col], format="%Y", errors="coerce").dt.strftime("%Y")

    # 7) í”¼ë²—ìœ¼ë¡œ í–‰, ì—´ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì •ë¦¬
    wide = (
        df2.pivot_table(index="date", columns=item_col, values=value_col, aggfunc="first")
           .sort_index()
    )

    # 8) í•­ëª©ëª… ì •ê·œí™”
    wide = wide.rename(columns=normalize_item)

    # 9) ì»¬ëŸ¼ ì„ íƒ
    wanted_cols = ["ê²½ì œí™œë™ì¸êµ¬", "ë¹„ê²½ì œí™œë™ì¸êµ¬", "ì·¨ì—…ì", "ì‹¤ì—…ì", "ì‹¤ì—…ë¥ ", "ê³ ìš©ë¥ ", "ê²½ì œí™œë™ì°¸ê°€ìœ¨"]
    final_cols = [c for c in wanted_cols if c in wide.columns]
    wide = wide[final_cols]

    wide = wide.reset_index()  # dateë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ

    # 10) ì €ì¥ (latest_ë‚ ì§œ íŒŒì¼ 1ê°œë§Œ ìœ ì§€)
    base_path = PROJECT_ROOT / cfg["output_csv"] 
    ensure_parent_dir(base_path)
    out_csv = replace_latest_dated_file(base_path) # ê¸°ì¡´ íŒŒì¼ ì—†ì• ê³  ìƒˆ íŒŒì¼ë¡œ ëŒ€ì²´í•œë‹¤ 

    wide.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # 11) metadata ê¸°ë¡
    key = cfg.get("metadata_key", "labor_force")
    update_meta(METADATA_PATH, key, {
        "saved_file": out_csv,
        "source_url": url,
        "rows": int(wide.shape[0]),
        "max_date": wide["date"].max() if not wide.empty else None,
    })

    print("âœ… Labor_Force ì €ì¥:", out_csv, "rows:", len(wide), "max_date:", (wide["date"].max() if not wide.empty else None))
    return {
        "saved_to": out_csv,
        "rows": int(wide.shape[0]),
        "max_date": wide["date"].max() if not wide.empty else None,
    } 

def working_index_run(cfg: dict):
    start_ym = cfg.get("start_ym", "202001")
    end_ym = cfg.get("end_ym")  # ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ê¸°ì¤€ YYYYMM
    if not end_ym:
        today = dt.date.today()
        end_ym = f"{today.year:04d}{today.month:02d}"

    
    raw = fetch_kosis_by_6m(
        openapi_url=cfg["openapi_url"],
        start_ym=start_ym,
        end_ym=end_ym,
        fetch_to_df=fetch_to_df,
        sleep_s=0.3
    ) # 40000ì…€ ì œí•œìœ¼ë¡œ ì¸í•´ 6ê°œì›” ë‹¨ìœ„ë¡œ ëŠì–´ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
    
    
    
    raw.columns = raw.columns.astype(str).str.strip()
    raw = raw[raw["C2_NM"] == "ì „ê·œëª¨(1ì¸ì´ìƒ)"].copy() # ê³„ ë°ì´í„°ë§Œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤ 

    date_col = "PRD_DE"
    item_col= "ITM_NM"
    value_col = "DT"
    
    wanted = ["ì „ì²´ì„ê¸ˆì´ì•¡", "ì „ì²´ê·¼ë¡œì¼ìˆ˜", "ì „ì²´ê·¼ë¡œì‹œê°„"]
    raw = raw[raw[item_col].isin(wanted)].copy()

    raw[value_col] = pd.to_numeric(raw[value_col], errors="coerce")
    raw[date_col] = raw[date_col].astype(str).str.strip()
    

    wide = (
        raw.pivot_table(index=date_col, columns=item_col, values=value_col, aggfunc="first")
          .sort_index()
    )
    wide = wide[[c for c in wanted if c in wide.columns]].reset_index()
    wide = wide.rename(columns={date_col: "date"})
    wide["date"] = pd.to_datetime(wide["date"], format="%Y%m").dt.strftime("%Y-%m")

    
    # 3) ì €ì¥
    out_csv = PROJECT_ROOT / cfg["output_csv"]
    ensure_parent_dir(out_csv)
    # 1) ì´ì „ latest_ë‚ ì§œ íŒŒì¼ ì œê±° +  íŒŒì¼ ê²½ë¡œ ìƒì„±
    out_csv = replace_latest_dated_file(out_csv)
    wide.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # 4) metadata ê¸°ë¡
    key = cfg.get("metadata_key", "working_index")
    update_meta(METADATA_PATH, key, {
        "saved_file": out_csv,
        "source_url": cfg["openapi_url"],
        "rows": int(wide.shape[0]),
        "max_date": wide["date"].max() if not wide.empty else None,
    })

    print("âœ… Working_Index ì €ì¥:", out_csv, "rows:", len(wide), "max_date:", wide["date"].max() if not wide.empty else None) 
    


def resident_population_run(cfg: dict):
    start_ym = str(cfg.get("start_ym", "200801")).strip()
    end_ym = cfg.get("end_ym")  # ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ê¸°ì¤€ YYYYMM
    if not end_ym:
        today = dt.date.today()
        end_ym = f"{today.year:04d}{today.month:02d}"
    end_ym = str(end_ym).strip()

    # raw ë¡œë”©
    raw = fetch_kosis_by_6m(
        openapi_url=cfg["openapi_url"],
        start_ym=start_ym,
        end_ym=end_ym,
        fetch_to_df=fetch_to_df,
        sleep_s=0.3
    ) # 40000ì…€ ì œí•œìœ¼ë¡œ ì¸í•´ 6ê°œì›” ë‹¨ìœ„ë¡œ ëŠì–´ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
    
    
    # ì „ì²˜ë¦¬
    raw.columns = raw.columns.astype(str).str.strip()

    # ì „êµ­ ë°ì´í„°ë§Œ
    #raw["C1_NM"] = raw["C1_NM"].astype(str).str.strip()
    #raw = raw[raw["C1_NM"] == "ì „êµ­"].copy()

    # í•„ìš”í•œ ì»¬ëŸ¼ ì²´í¬
    need = ["PRD_DE", "ITM_NM", "C2_NM", "DT"]
    miss = [c for c in need if c not in raw.columns]

    # ë¬¸ìì—´ ì •ë¦¬ + ê°’ ìˆ«ìí™”
    for c in ["PRD_DE", "ITM_NM", "C2_NM"]:
        raw[c] = raw[c].astype(str).str.strip()
    raw["DT"] = pd.to_numeric(raw["DT"], errors="coerce")

    # ì´ì¸êµ¬ìˆ˜ë§Œ ì‚¬ìš©
    raw = raw[raw["ITM_NM"] == "ì´ì¸êµ¬ìˆ˜"].copy()

    # ì—°ë ¹ 'ê³„' ì œê±°(ìˆìœ¼ë©´)
    raw = raw[raw["C2_NM"] != "ê³„"].copy()
    
    # ì—°ë ¹ íŒŒì‹±: "0ì„¸" -> 0, "100ì„¸ ì´ìƒ" -> 100
    def parse_age(s: str):
        s = str(s)
        m = re.search(r"(\d+)", s)
        return int(m.group(1)) if m else None

    raw["age"] = raw["C2_NM"].apply(parse_age)
    raw = raw.dropna(subset=["age"]).copy()
    raw["age"] = raw["age"].astype(int)
    
    # ì›”ë³„(YYYYMM)ë¡œ ì—°ë ¹ëŒ€ í•©ê³„ + ì´ì¸êµ¬(ë¶„ëª¨: ì—°ë ¹í•©)
    def agg(g: pd.DataFrame) -> pd.Series:
        return pd.Series({
            "ì´ì¸êµ¬ìˆ˜": g["DT"].sum(),
            "pop_0_14": g.loc[g["age"].between(0, 14), "DT"].sum(),
            "pop_15_64": g.loc[g["age"].between(15, 64), "DT"].sum(),
            "pop_65p": g.loc[g["age"] >= 65, "DT"].sum(),
        })

    out = raw.groupby("PRD_DE").apply(agg).reset_index().sort_values("PRD_DE")

    # êµ¬ì„±ë¹„(%)
    out["0~14ì„¸ êµ¬ì„±ë¹„"] = out["pop_0_14"] / out["ì´ì¸êµ¬ìˆ˜"] * 100
    out["15~64ì„¸ êµ¬ì„±ë¹„"] = out["pop_15_64"] / out["ì´ì¸êµ¬ìˆ˜"] * 100
    out["ê³ ë ¹ì¸êµ¬ë¹„ìœ¨"] = out["pop_65p"] / out["ì´ì¸êµ¬ìˆ˜"] * 100
    
    out = out.drop(columns=["pop_0_14", "pop_15_64", "pop_65p"])

    # ë‚ ì§œ í¬ë§· ì •ë¦¬ (YYYYMM -> YYYY-MM)
    out = out.rename(columns={"PRD_DE": "date"})
    out["date"] = pd.to_datetime(out["date"], format="%Y%m").dt.strftime("%Y-%m")

    out.reset_index()
    # ì €ì¥ 

    out_csv = PROJECT_ROOT / cfg["output_csv"]
    ensure_parent_dir(out_csv)
    out_csv = replace_latest_dated_file(out_csv)
    out.to_csv(out_csv, index=False, encoding="utf-8-sig")
    # ë©”íƒ€ë°ì´í„° ê¸°ë¡
    key = cfg.get("metadata_key", "resident_population")
    update_meta(METADATA_PATH, key, {
        "saved_file": out_csv,
        "source_url": cfg.get("openapi_url"),
        "rows": int(out.shape[0]),
        "max_date": out["date"].max() if not out.empty else None,
    })
    
    print("âœ… Resident_Population ì €ì¥:", out_csv, "rows:", len(out), "max_date:", out["date"].max())
    


COLLECTOR_MAP = {
    "cpi": cpi_run,
    "consumer_price_change_index": consumer_price_change_index_run,
    "average_working_day": average_working_day_run,
    "aver_mid_age": aver_mid_age_run,
    "loan": loan_run,
    "gdp_gni": gdp_gni_run,
    "labor_force": labor_force_run,
    "working_index": working_index_run,
    "resident_population": resident_population_run,
}
# ============================================================
# ğŸ“¦ concat_database
# ============================================================
def find_min_data(metadata_path = METADATA_PATH):
    with open(metadata_path, "r", encoding="utf-8") as f:
        meta = json.load(f)
    max_dates =[]
    for k,v in meta.items():
        if "max_date" in v and v["max_date"]:
            max_dates.append(pd.to_datetime(v["max_date"],format="%Y-%m",errors="raise"))
    common_min_date = min(max_dates)
    #print("ê¸°ì¤€ ì›”",common_min_date)
    return common_min_date 

def load_and_trim_monthly(csv_path, start_date, end_date, date_col="date"):
    df = pd.read_csv(csv_path)

    # 1) ë‚ ì§œ â†’ Timestamp (í˜¼í•© í¬ë§· ì•ˆì „)
    df[date_col] = pd.to_datetime(df[date_col], format="mixed", errors="raise")

    start_date = pd.to_datetime(start_date, format="mixed")
    end_date = pd.to_datetime(end_date, format="mixed")

    # 2) ê¸°ê°„ í•„í„°ë§ (Timestampë¼ë¦¬ ë¹„êµ)
    df = df[(df[date_col] >= start_date) & (df[date_col] <= end_date)]

    # 3) ìµœì¢… í¬ë§· í†µì¼ (ë¬¸ìì—´ë¡œ ë°”ê¾¸ëŠ” ê±´ ë§ˆì§€ë§‰ì—)
    df[date_col] = df[date_col].dt.strftime("%Y-%m")

    return df.sort_values(date_col).reset_index(drop=True)

def merge_all_monthly_from_metadata(metadata_path = METADATA_PATH, start_date="2020-01", date_col="date"):
    with open(metadata_path, "r", encoding="utf-8") as f:
        meta = json.load(f)

    dfs = []
    common_min_date = find_min_data()
    for key, info in meta.items():
        if key == "suicide_base_data":
            continue
        csv_path = info["saved_file"]

        df = load_and_trim_monthly(
            csv_path=csv_path,
            start_date=start_date,
            end_date=common_min_date,
            date_col=date_col
        ) # start ~ end ê¸°ê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ ìë¥¸ë‹¤ 
        dfs.append(df)

    # inner join: ê³µí†µ ê¸°ê°„ë§Œ ë‚¨ê¹€
    df_merged = dfs[0]
    for df in dfs[1:]:
        df_merged = df_merged.merge(df, on=date_col, how="inner")

    return df_merged.sort_values(date_col).reset_index(drop=True)

def concat_database_run(cfg: dict):
    start_date = cfg.get("start_date", "2020-01")

    # í…œí”Œë¦¿ì€ ë¬¸ìì—´ë¡œ ë¨¼ì € format
    output_csv_tpl = cfg["output_csv"].format(max_year="{max_year}")  # ì•ˆì „ì¥ì¹˜ (ì´ë¯¸ {max_year}ê°€ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ)
    metadata_key = cfg.get("metadata_key", "suicide_base_data")

    df = merge_all_monthly_from_metadata(start_date=start_date)

    max_year = pd.to_datetime(df["date"]).dt.year.max()

    # ğŸ”¥ íŒŒì¼ëª… ë™ì  ì¹˜í™˜ (ë¬¸ìì—´ â†’ Path)
    rel_path = cfg["output_csv"].format(max_year=max_year)
    out_csv = PROJECT_ROOT / rel_path

    # 3) ì €ì¥ (ë””ë ‰í„°ë¦¬ ìƒì„± â†’ latest ë‚ ì§œ íŒŒì¼ë¡œ êµì²´)
    ensure_parent_dir(out_csv)
    out_csv = replace_latest_dated_file(out_csv)
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # 4) metadata ê¸°ë¡ (ğŸ”¥ Timestamp â†’ ë¬¸ìì—´ ë³€í™˜)
    max_date_str = (
        pd.to_datetime(df["date"]).max().strftime("%Y-%m")
        if not df.empty else None
    )

    update_meta(METADATA_PATH, metadata_key, {
        "saved_file": out_csv,
        "rows": int(df.shape[0]),
        "max_date": max_date_str,     # JSON ì§ë ¬í™” ì•ˆì „
        "start_date": start_date,     # ë¬¸ìì—´  
    })

    print(
        "âœ… Suicide_Base_Data ì €ì¥:",
        out_csv,
        "rows:", len(df),
        "max_date:", max_date_str,
    )
    """
    start_date = cfg.get("start_date", "2020-01")
    
    output_csv_tpl = PROJECT_ROOT / cfg["output_csv"]            # "../data/suicide_base_data_2020_{max_year}.csv"
    metadata_key = cfg.get("metadata_key", "suicide_base_data")

    df = merge_all_monthly_from_metadata(start_date=start_date)

    max_year = pd.to_datetime(df["date"]).dt.year.max()

    # ğŸ”¥ íŒŒì¼ëª… ë™ì  ì¹˜í™˜
    out_csv = output_csv_tpl.format(max_year=max_year)

    # 3) ì €ì¥ (ê¸°ì¡´ collector ìŠ¤íƒ€ì¼ ê·¸ëŒ€ë¡œ)
    ensure_parent_dir(out_csv)
    out_csv = replace_latest_dated_file(out_csv)
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")

    # 4) metadata ê¸°ë¡ (ğŸ”¥ Timestamp â†’ ë¬¸ìì—´ ë³€í™˜)
    max_date_str = (
        pd.to_datetime(df["date"]).max().strftime("%Y-%m")
        if not df.empty else None
    )

    update_meta(METADATA_PATH, metadata_key, {
        "saved_file": out_csv,
        "rows": int(df.shape[0]),
        "max_date": max_date_str,     # âœ… JSON ì§ë ¬í™” ì•ˆì „
        "start_date": start_date,     # ë¬¸ìì—´  
    })

    print(
        "âœ… Suicide_Base_Data ì €ì¥:",
        out_csv,
        "rows:", len(df),
        "max_date:", max_date_str,
    )
    """